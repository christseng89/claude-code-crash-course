# New File: README-ContextEngineering.md

## Context Engineering

CONTEXT WINDOW (Limited & Expensive)

GOOD CONTEXT             vs           BAD CONTEXT
âœ“ Fast Performance                    âœ— Poor Performance
âœ“ Accurate Results                    âœ— Wasted Tokens
âœ“ Lower Cost                          âœ— Higher Latency/Cost

### ğŸ“„ Extracted Text (Preserved Layout)

```md
CONTEXT WINDOW (200k tokens)

[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]
 â†‘ MCP Tools (50k+ tokens)                  â†‘ Actual Work (150k tokens)

WASTED BEFORE YOU EVEN START! ğŸ˜±

Tools Loaded but NOT Used:

[ Tool1 ]   [ Tool2 ]   [ Tool3 ]   [ Tool4 ]   [ Tool5 ]
    âœ—          âœ—          âœ—          âœ—          âœ—

[ Tool6 ]   [ Tool7 ]   [ Tool8 ]   [ Tool9 ]   [  ...  ]
    âœ—          âœ—          âœ—          âœ—          âœ—
```

---

### ğŸ§  Why This Matters

#### 1ï¸âƒ£ Context Window Is **Limited & Expensive**

* The LLM has a fixed **context window** (here: **200k tokens**)
* Every token used:

  * Costs money ğŸ’°
  * Increases latency â³
  * Reduces room for real reasoning

---

#### 2ï¸âƒ£ MCP Tools Are Loaded **Before Any Work Starts**

* MCP tools + schemas + descriptions can consume:

  * **50k+ tokens**
* This happens **even if the tools are never called**

Result:

> âš ï¸ **25% of your context is gone before the model reasons at all**

---

#### 3ï¸âƒ£ â€œActual Workâ€ Gets Squeezed

* Only **150k tokens** remain for:

  * User input
  * Reasoning
  * Planning
  * Output

Which leads to:

* Shorter reasoning chains
* Less accurate answers
* More hallucinations

---

#### 4ï¸âƒ£ Unused Tools = Wasted Tokens

All of these:

```md
Tool1, Tool2, Tool3, Tool4, Tool5, Tool6, Tool7, Tool8, Tool9, ...
```

âŒ Were **loaded**
âŒ Were **never used**
âŒ Still consumed tokens

Thatâ€™s **pure waste**.

---

### ğŸš¨ Core Message of the Diagram

> **More tools â‰  better AI**
> **Right tools, loaded at the right time = better AI**

---

## âœ… MCP Servers Best Practices

### âœ” Lazy-load tools

Only load MCP tools **when needed**

### âœ” Split MCP servers by domain

* Docs MCP
* DB MCP
* Web MCP
* Code MCP

### âœ” Context minimization

* Smaller schemas
* Short tool descriptions
* No â€œalways-onâ€ tools

---

## ğŸ§© Why This Is Especially Relevant to MCP

MCP makes tool usage easy â€” but:

> â— MCP does **not** automatically solve context bloat
> â— Tool orchestration still matters

This diagram is warning against:

* â€œConnect everythingâ€ MCP servers
* Overloaded tool registries
* One giant agent with all tools

---

## ğŸ§  One-Sentence Summary

> If you load 50k tokens of tools you donâ€™t use **before running a simple prompt**, youâ€™ve already wasted context, money, and reasoning capacity before the AI even starts thinking.

---

## The Core Problem (.mcp.json)

If 50k tokens of unused tools are loaded before executing even a simple prompt, context, cost, and reasoning capacity are already wasted before the model begins processing.

## Hands on

```bash
cd context-engineering-mcp
uv sync
uv run verbose_mcp_server.py

source .venv/Scripts/activate

fastmcp --help
fastmcp run verbose_mcp_server.py --transport http

```

```json (.mcp.json in parent directory)
      "verbose-server": {
        "type": "http",
        "url": "http://127.0.0.1:8000/mcp"
      },
```

```bash
cd context-engineering-mcp
claude --mcp-config ../.mcp.json

/mcp
/context

```

```md
â¿  Context Usage
     â› â› â› â› â› â› â› â› â› â›   claude-sonnet-4-5-20250929 Â· 50k/200k tokens (25%)                                       
     â› â› â› â› â› â› â› â›€ â› â›                                                                                            
     â› â› â› â› â›€ â›€ â›€ â›¶ â›¶ â›¶   Estimated usage by category                                                                   â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶   â› System prompt: 2.8k tokens (1.4%)            
     â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶   â› System tools: 16.6k tokens (8.3%)                                                      
     â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶   â› MCP tools: **17.1k** tokens (8.5%)
     â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶   â› Memory files: 13.0k tokens (6.5%)
     â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶ â›¶   â› Skills: 386 tokens (0.2%)
     â›¶ â›¶ â›¶ â› â› â› â› â› â› â›   â› Messages: 8 tokens (0.0%)
     â› â› â› â› â› â› â› â› â› â›   â›¶ Free space: 117k (58.6%)
                           â› Autocompact buffer: 33.0k tokens (16.5%)

     MCP tools Â· /mcp
     â”” mcp__verbose-server__add_two_numbers: 327 tokens
     â”” mcp__verbose-server__subtract_two_numbers: 364 tokens
     â”” mcp__verbose-server__multiply_two_numbers: 439 tokens
      ...
     â”” mcp__playwright__browser_tabs: 154 tokens
     â”” mcp__playwright__browser_wait_for: 152 tokens

```

```bash
# Option 1
/exit
claude --mcp-config ../.mcp.json.verbose --strict-mcp-config
/mcp
/context

# Option 2
/exit
claude --mcp-config ../.mcp.json --strict-mcp-config

/mcp

   Built-in MCPs (always available)
 â¯ context7 Â· âœ” connected
   github Â· âœ” connected
   playwright Â· âœ” connected
   verbose-server Â· âœ” connected

 â¯ 3. Disable  

   Built-in MCPs (always available)
 â¯ context7 Â· â—¯ disabled
   github Â· â—¯ disabled
   playwright Â· â—¯ disabled 
   verbose-server Â· âœ” connected    

/exit

claude --mcp-config ../.mcp.json 
/mcp
/context    
```
